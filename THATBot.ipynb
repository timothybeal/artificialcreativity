{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "THATBot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timothybeal/artificialcreativity/blob/master/THATBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN4Ehli4Dqzc",
        "colab_type": "text"
      },
      "source": [
        "# **MAKE A BOT!**\n",
        "\n",
        "This notebook was created for a \"make\" session of THATCamp at the annual meeting of the American Theological Library Association. It is a fully functional bot, built in the programming language of Python, that uses Markov processes to autogenerate its own verses based on the King James Version Bible. This is a slightly simplified version of the program behind [KJVBot](https://twitter.com/kjvbot), which tweets its auto-generated verses based on all or part of the KJV Bible.\n",
        "<br>\n",
        "<br>\n",
        "This program can easily be adapted to work from other texts (just upload a different .txt file and feed it start phrases that you know are in that text). No coding experience is necessary here, but each step is annotated for those who want to understand more about what's going on from line to line.\n",
        "<br>\n",
        "<br>\n",
        "As with all programs in Colab, you can either run each cell of code one at a time by clicking each play button, or you can run them all in order by selecting \"Runtime\" / \"Run all\" from the menu at the top.\n",
        "\n",
        "##**About Markov Processes**\n",
        "\n",
        "The Markov process is a simple yet powerful means of prediction. It begins with a *current state* and, based on probability, predicts what the *next state* to follow that state will be. So, for example, if we were talking about predicting the weather, and the *current state* were raining, the Markov process would make a list of all the states in its data that have come after raining (partly cloudy, sunny, sunny, raining, raining, raining, partly cloudy). Then it would randomly choose from that list, and whatever it chooses would become the new *current state*. Given that certain next states are more common in the list than others (e.g., continuing rain), its prediction is probablistic. Now, imagine that our database is not weather history but *The Washington Post*, and that our *current state* is the word \"Barack.\" The Markov process would go through the entire database of news text (very quickly!), make a list of every *next state* word or punctuation mark that has followed \"Barack,\" and randomly select one from that list. Let's say 96% of the time \"Obama\" is the *next state* word after \"Barack.\" So most of the time the process will end up selecting that word as its *next state* prediciton. That word then becomes the *current state*, and the process begins again, compiling and selecting from a list of all the *next states* to \"Obama\". And so on.\n",
        "<br>\n",
        "<br>\n",
        "With this bot, the process continues until it lands upon a period, exclamation mark, question mark, at which point it stops and, if the length of the utterance is less than 130 characters (or whatever you set the limit to be), it prints that utterance. If it's over the set limit, it starts again from scratch. And all that in a second or two!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLOfelDXFXK7",
        "colab_type": "text"
      },
      "source": [
        "##**1. Import the needed libraries.**\n",
        "\n",
        "Libraries are collections of pre-written code that can perform certain functions that are imported and used in a program. The ones we will use are:\n",
        "\n",
        "> **files** (part of Google's **colab** library), which allows us to upload the text file from Drive or a local computer into this program;\n",
        "<br>\n",
        "<br>\n",
        "> **nltk** (\"Natural Language Tool Kit\" library), which we use to convert our text from a single string of words and punctuation into a list of sentences and then, using **regular expressions**, below, we turn those sentences into lists of tokens, each of which is a single word or punctuation mark (e.g., \"[\"and\", \",\", \"behold\"]\");\n",
        "<br>\n",
        "<br>\n",
        "> **tee** (part of the **itertools** library), which makes a list of sequences, with each sequence moving ahead one step (see below in the list_crawler function);\n",
        "<br>\n",
        "<br>\n",
        "> **defaultdict** (part of the **collections** library), which works with **tee** to make a dictionary (a collection of key:value pairs) that will give us every next word (*next state*) that follows every three-word string (*current state*) in our text;\n",
        "<br>\n",
        "<br>\n",
        "> **re** is the **regular expressions** library, which we use to tokenize the text and to find and replace certain characters and line breaks in the text; and \n",
        "<br>\n",
        "<br>\n",
        "> **choice** (part of the **random** library), which randomly chooses an item from a list (we use it to randomly choose the bot's starting point from a list of possibilities).\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of5rjES_FqiN",
        "colab_type": "code",
        "outputId": "f2912b34-0957-4f63-e02c-0463448bb451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import files\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from itertools import tee\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from random import choice\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu9-lpZvM7zq",
        "colab_type": "text"
      },
      "source": [
        "##**2. Upload a text for the bot to work from.**\n",
        "\n",
        "In Colab, you need to upload whatever files you need to run your program every time you start a new session. For the purposes of this exercise, we will upload a plain text version of the King James Version Bible. When you run this line, a pop up menu will appear. Find your kjv.txt file in Drive or wherever you have it saved. Colab will then upload it for use during this session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me7RG5C4M5mv",
        "colab_type": "code",
        "outputId": "c4eeea75-1d73-4004-9789-a8d17b7c10e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ed2fd71b4a2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvnuCA5UP6H4",
        "colab_type": "text"
      },
      "source": [
        "##**3. Define the three functions needed to run the bot.**\n",
        "\n",
        "Functions work like programs within a program to perform certain actions. Most often, certain functions are run, or \"called,\" within other functions. So, in what follows, the first two functions, list_crawler() and build_sentence(), are called inside the third, markovize().\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJZTVcWQfQ-",
        "colab_type": "text"
      },
      "source": [
        "**A. Define the list_crawler() function.**\n",
        "\n",
        "These lines build a function that crawls through a list -- in this case, the KJV text as a list of words and punctuation marks -- moving forward one step each time. So, if our text were Genesis 1, it would result in a list like this: [(\"in\", \"the\", \"beginning\"), (\"the\", \"beginning\", \"god\"), (\"beginning\", \"god\", \"created\"), (\"god\", \"created\", \"the\")]. This function will be used (\"called\") in the markovize() function to find all *next states* for all *current states* (see below).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-QQijaery3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_crawler(iterable, n=2):\n",
        "    if len(iterable) < n:\n",
        "        return\n",
        "    iterables = tee(iterable, n)\n",
        "    for i, iter_ in enumerate(iterables):\n",
        "        for num in range(i):\n",
        "            next(iter_)\n",
        "    return zip(*iterables)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUv-N-_f-Iw",
        "colab_type": "text"
      },
      "source": [
        "**B. Define the build_sentence() function.**\n",
        "\n",
        "These lines will be used along with list_crawler() inside the markovize() function, below, to create the new utterance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39j3uRw1udHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_sentence(seed, sent_tokens):\n",
        "    token = ''\n",
        "    while token not in set('׃.?!\\n'):\n",
        "        last_tokens = tuple(seed[-3:])\n",
        "        new_token = choice(sent_tokens[last_tokens])\n",
        "        seed.append(new_token)\n",
        "        token = new_token\n",
        "    sentence = ' '.join(seed)\n",
        "    sentence = re.sub(r'\\s+([׃.,?!:;\\n])', r'\\1', sentence)\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8AphkZAs_Ur",
        "colab_type": "text"
      },
      "source": [
        "**C. Define the markovize() function.**\n",
        "\n",
        "This function does a lot. First, it opens the text we uploaded (lines 2-3). Then it turns that text into a list of separated sentences (line 4) and tokens (lines 6-7). Then, using the list_crawler() function, defined above, and the **defaultdict** module that we imported, it turns that list of tokens into a huge dictionary of key:value pairs, with each \"key\" being a three-token string and each \"value\" being the next word that follows that string in the text. What this gives us, then, is every next word, or *next state*, that comes after every three-word phrase, or *current state*, in the text. The result of this process gives us a very, very long dictionary that looks like this (with the three tokens in parentheses as the key and the bracketed token, which is the *next state* in the text, is the value for that key):\n",
        "\n",
        ">{('the', 'revelation', 'of'): ['jesus'], ('revelation', 'of', 'jesus'): ['christ'], ('of', 'jesus', 'christ'): [','], ('jesus', 'christ', ','): ['which'], ('christ', ',', 'which'): ['god'], (',', 'which', 'god'): ['gave'] ...}\n",
        "\n",
        "Once it has built that dictionary, the build_sentence() function works within an iterating loop to build the actual verse or \"utterance.\" It does so using a Markov process: beginning with a three-token start phrase as its *current state*, it makes a list of all possible *next states* (tokens that follow that phrase) in the text, and then randomly selects one from its list. That new token then becomes the third of the three tokens in the *current state* (the former first token drops off), and the process begins again. The process continues until it randomly selects a period, exclamation point, or question mark, at which point it stops. If the resulting utterance is less than 130 characters (or whatever you set the limit to be), it prints it; if not, it starts all over again with a new three-token start phrase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MabOs0O5gPsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def markovize(word1, word2, word3, fileid, char_limit=None):\n",
        "    with open(fileid, encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    sentences = sent_tokenize(text)\n",
        "    sent_tokens = defaultdict(list)\n",
        "    for sentence in sentences:\n",
        "        tokens = re.findall(r\"[\\w']+|[׃.,?!:;\\n]\", sentence)\n",
        "        crawled_list = list_crawler(tokens, n=4)\n",
        "        if crawled_list:\n",
        "            for token1, token2, token3, token4 in crawled_list:\n",
        "                sent_tokens[token1, token2, token3].append(token4)\n",
        "    too_long = True\n",
        "    while too_long:\n",
        "        sentence = [word1, word2, word3]\n",
        "        utterance = build_sentence(sentence, sent_tokens)\n",
        "        len_utterance = len(utterance)\n",
        "        if char_limit is not None and len_utterance > char_limit:\n",
        "            too_long = True\n",
        "        else:\n",
        "            too_long = False\n",
        "    print(utterance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmzPBK2vynSE",
        "colab_type": "text"
      },
      "source": [
        "##**4. Create several different start phrases for the bot.**\n",
        "\n",
        "Here we simply create a list of possible three-token starting points for the bot. Note that they all need to show up someplace in the KJV text or the bot will fail before it begins. We use the **choice** module that we imported to randomly choose the starting point for the bot each time it runs.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ73FjwuzLA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_phrases = [[\"Woe\", \"unto\", \"the\"],\n",
        "     [\"And\", \"when\", \"he\"],\n",
        "     [\"And\", \"I\", \"saw\"],\n",
        "     [\"And\", \"he\", \"answered\"],\n",
        "     [\"And\", \"the\", \"priest\"],\n",
        "     [\"In\", \"the\", \"beginning\"]]\n",
        "\n",
        "[word1, word2, word3] = choice(start_phrases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJcY4m5nzmhT",
        "colab_type": "text"
      },
      "source": [
        "##**5. Run the bot!**\n",
        "\n",
        "We run the bot by calling the function markovize(), which, as we saw, incorporates the two previous functions within it. We do that simply by typing its name plus the details (\"key arguments\") it needs to run, namely: the first three tokens (word1, word2, word3, which have been randomly selected from the five choices for \"start_phrases\", above), the text file it will process (kjv.txt), and the character limit for the utterance (130).\n",
        "<br>\n",
        "<br>\n",
        "After you've run the whole program once during a session, you can simply run the last two cells to produce new utterances with new start phrases (or run only the last cell to rerun the bot with the same start phrase).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jZDoZ5q090U",
        "colab_type": "code",
        "outputId": "3583c292-bd34-4766-d753-a9cf9ae1bba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "markovize(word1, word2, word3, \"kjv.txt\", 130)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And he answered, No.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}